{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EE-361M Introduction to Data Mining\n",
    "## Assignment #5\n",
    "## Due: Thursday, Apr 14, 2016 by 2pm; Total points: 60\n",
    "\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook** (if this isn't possible, let me know). Please use this naming format for your notebook you submit: **Group(Group Num)_HW(HW Number).ipynb**. For example, Group1_HW1.ipynb. Homeworks should be submitted through Canvas in your **groups of 3 from the first homework**. If groups need to be adjusted please contact the TA. Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 (1+1+5+3+3+2=15pts) - Logistic Regression\n",
    "\n",
    "In this question we will be predicting whether someone will have an affair! Yes - there is data on this. See below on how to import the data.\n",
    "1. Convert naffairs to a binary variable hadAffair which is 1 if had an affair and zero otherwise\n",
    "2. Split the data into training and test. Use 42 as random seed and use 1/3rd of the data for testing. Our y variable is hadAffair and X matrix includes all the other variables except naffairs.\n",
    "3. Train a logistic regression with almost no regularization (pass l2 (ridge) to penalty and 1,000 to the C parameter which is the inverse of regularization strength lambda. This essentially does l2 regularization but applies very little weight to the penalty term) and report the confusion matrix on the test data. Also report the accuracy for the \"no affairs\" class, the affairs class, and the average per-class accuracy on the test data. Average per-class accuracy is described in this [post](http://blog.dato.com/how-to-evaluate-machine-learning-models-part-2a-classification-metrics).\n",
    "4. Repeat step 3 except use l2 penalty with Cs of [.001, .01,0.1, 1]. You will want to use k-fold cross validation to select the best parameter. To evaluate which parameter is best, maximize the average per-class accuracy. To help with this task, check out [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) and how to make your own [custom scorer](http://scikit-learn.org/stable/modules/model_evaluation.html).\n",
    "5. Repeat question 4 except use l1 (i.e. Lasso) instead of l2 as the penalty type.\n",
    "6. Which model produces the best average per-class accuracy? Why do you think this is the case? How do the models handle the different classes, and why is this so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pydataset import data\n",
    "df = data('affairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 (2+3+2+3=10pts) - Support Vector Classifier\n",
    "\n",
    "This question will continue to use the data from question 1 - including the training and test split data.\n",
    "1. Fit a support vector classifier using the standard options on [sklearn's SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC). Report the confusion matrix on the test data. Also report the accuracy for the no affairs class, the affairs class, and the average per-class accuracy (same as question 1).\n",
    "2. Repeat question 1 except use grid search to select the best value of C within this set: [0.001, 0.01, 0.1, 1,5,10,100] and try both a radial and polynomial kernel (thus trying 14 combinations). Choose the combination that maximizes the average per-class accuracy. Use 5 folds. Report the best model, the confusion matrix, the accuracy for the no affairs class, the affairs class, and the average per-class accuracy.\n",
    "3. Briefly discuss the effect of different  C,  kernel combinations.\n",
    "4. Discuss your results from parts 1 and 2 and mention how they differ from Question 1's results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 (2+1+3+1+3=10pts) - Regression Trees\n",
    "\n",
    "This question is very similar to homework 4 question 1. Except now we will be using regression trees and not classification trees, and you will be addressing a regression problem (i.e., the independent variable \"price\" will not be binarized).\n",
    "\n",
    "For this question, we will be using the housing dataset (see code below). \n",
    "\n",
    "1. Convert driveway, recroom, fullbase, gashw, airco, and prefarea to numeric dummy variables (1 for yes, zero for no)\n",
    "2. Split the data into training and testing with a random seed of 42 and keeping 1/3rd of the data for testing\n",
    "2. Fit a [decision tree regressor](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor) to predict price using all the data (your dummy variables plus bedrooms and bathrooms).\n",
    "5. Report the root MSE on the test data.\n",
    "6. How does the tree decide on a splitting point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = data('Housing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 (2+5+3=10pts) - Support Vector Regression\n",
    "\n",
    "This question will continue to use the data from question 3 - including the training and test split data.\n",
    "\n",
    "1. Fit a support vector regression using the standard options on [sklearn's SVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html). Report the root MSE.\n",
    "2. Repeat question 1 except use grid search to select the best value of C within this set: [0.001, 0.01, 0.1, 1,5,10,100] and try both a radial and polynomial kernel (thus trying 14 combinations). Choose the combination that minimizes MSE. Use 5 folds. Report the best model and the test root MSE.\n",
    "4. Discuss your results from parts 1 and 2 and how they differ from Question 3 results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 (3+2+2+2+2+2+2=15pts) - Random Forest\n",
    "\n",
    "This question will also continue to use the data from Question 1.\n",
    "1. Fit a random forest model grid searching over the following values: {'n_estimators':[10, 100, 1000], 'max_features': ['auto', 'sqrt', 'log2']}. Choose the combination that maximizes the average per-class accuracy. Use 5 folds. Report the best model, the confusion matrix, the accuracy for the no affairs class, the affairs class, and the average per-class accuracy.\n",
    "2. What do the n_estimators and max_features parameters do?\n",
    "3. Report the features in order of importance based on the model used in part 1\n",
    "4. Repeat question 1 using an AdaBoostClassifier and grid search over the following values: {'n_estimators':[50, 500, 5000], 'learning_rate': [.001, .01, .1]}\n",
    "5. What does the learning_rate parameter do?\n",
    "6. Report the features in order of importance based on the model used in part 4\n",
    "7. Compare the results in part 1 and 4 and questions 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "The confusion matrix is:\n",
      " [[80 11]\n",
      " [25  5]]\n",
      "The accuracy for no affair class is: 0.8791208791208791\n",
      "The accuracy for affair class is: 0.16666666666666666\n",
      "The average per-class accuracy is: 0.5228937728937729\n",
      "[ 0.10683433  0.05587136  0.06854369  0.0522069   0.05583066  0.07528018\n",
      "  0.06950056  0.0582785   0.06009735  0.05871943  0.03493494  0.03357845\n",
      "  0.04579172  0.04823245  0.05820833  0.05633889  0.06175225]\n"
     ]
    }
   ],
   "source": [
    "#5.1\n",
    "from pydataset import data\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import cross_validation, svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def convert_dummy(col,row):\n",
    "    if row[col] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df = data('affairs')\n",
    "df['naffairs'] = df.apply(lambda row: convert_dummy('naffairs',row),axis=1)\n",
    "X = df[['kids', 'vryunhap','unhap','avgmarr','hapavg','vryhap','antirel','notrel','slghtrel','smerel',\n",
    "          'vryrel','yrsmarr1','yrsmarr2','yrsmarr3','yrsmarr4','yrsmarr5','yrsmarr6']].values\n",
    "y = df.naffairs.values\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 100, 1000],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rs = GridSearchCV(clf, param_grid=param_grid)\n",
    "rs.fit(X,y)\n",
    "print(rs.best_estimator_)\n",
    "# 5.1 Best estimator is max_features=auto, n_estimator=1000\n",
    "X_train,X_test,y_train,y_test = cross_validation.train_test_split(\n",
    "     X, y, test_size=0.2, random_state=42)\n",
    "clf = RandomForestClassifier(n_estimators=1000, max_features='auto')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "naffair, nacorr, waffair, wacorr = 0, 0, 0, 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == 0:\n",
    "        naffair = naffair + 1\n",
    "        if y_pred[i] == 0:\n",
    "            nacorr = nacorr + 1\n",
    "    else:\n",
    "        waffair = waffair + 1\n",
    "        if y_pred[i] == 1:\n",
    "            wacorr = wacorr + 1\n",
    "\n",
    "naaccu = float(nacorr) / naffair\n",
    "waaccu =  float(wacorr)/waffair\n",
    "perclassaccu = (naaccu + waaccu) / 2\n",
    "print(\"The confusion matrix is:\\n\", confusion_matrix(y_test,y_pred))\n",
    "print(\"The accuracy for no affair class is:\", naaccu)\n",
    "print(\"The accuracy for affair class is:\", waaccu)\n",
    "print(\"The average per-class accuracy is:\", perclassaccu)\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "print(importances)\n",
    "# 5.2\n",
    "# N_estimators shows the number of trees in the forest.\n",
    "# Max_features shows the number of features to consider when looking for the best split:\n",
    "# 5.3\n",
    "# From the weight table, we can see that features are ranked as follows in terms of weight:\n",
    "# (from most to least important) kids, vryhap, antirel, unhap,  ... yrsmarr2, vryrel, yrsmarr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.001, n_estimators=5000, random_state=None)\n",
      "The confusion matrix is:\n",
      " [[88  3]\n",
      " [29  1]]\n",
      "The accuracy for no affair class is: 0.967032967032967\n",
      "The accuracy for affair class is: 0.03333333333333333\n",
      "The average per-class accuracy is: 0.5001831501831502\n",
      "[ 0.037   0.1432  0.1688  0.      0.      0.1358  0.1406  0.      0.      0.117\n",
      "  0.0184  0.0624  0.1768  0.      0.      0.      0.    ]\n"
     ]
    }
   ],
   "source": [
    "#5.4\n",
    "from pydataset import data\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import cross_validation, svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def convert_dummy(col,row):\n",
    "    if row[col] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df = data('affairs')\n",
    "df['naffairs'] = df.apply(lambda row: convert_dummy('naffairs',row),axis=1)\n",
    "X = df[['kids', 'vryunhap','unhap','avgmarr','hapavg','vryhap','antirel','notrel','slghtrel','smerel',\n",
    "          'vryrel','yrsmarr1','yrsmarr2','yrsmarr3','yrsmarr4','yrsmarr5','yrsmarr6']].values\n",
    "y = df.naffairs.values\n",
    "\n",
    "clf = AdaBoostClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 500, 5000],\n",
    "    'learning_rate': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "rs = GridSearchCV(clf, param_grid=param_grid)\n",
    "rs.fit(X,y)\n",
    "print(rs.best_estimator_)\n",
    "# 5.1 Best estimator is learning_rate = 0.001, n_estimator=5000\n",
    "\n",
    "X_train,X_test,y_train,y_test = cross_validation.train_test_split(\n",
    "     X, y, test_size=0.2, random_state=42)\n",
    "clf = AdaBoostClassifier(n_estimators=5000, learning_rate=0.001)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "naffair, nacorr, waffair, wacorr = 0, 0, 0, 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == 0:\n",
    "        naffair = naffair + 1\n",
    "        if y_pred[i] == 0:\n",
    "            nacorr = nacorr + 1\n",
    "    else:\n",
    "        waffair = waffair + 1\n",
    "        if y_pred[i] == 1:\n",
    "            wacorr = wacorr + 1\n",
    "\n",
    "naaccu = float(nacorr) / naffair\n",
    "waaccu =  float(wacorr)/waffair\n",
    "perclassaccu = (naaccu + waaccu) / 2\n",
    "print(\"The confusion matrix is:\\n\", confusion_matrix(y_test,y_pred))\n",
    "print(\"The accuracy for no affair class is:\", naaccu)\n",
    "print(\"The accuracy for affair class is:\", waaccu)\n",
    "print(\"The average per-class accuracy is:\", perclassaccu)\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "print(importances)\n",
    "# 5.5\n",
    "# Learning rate shrinks the contribution of each classifier by learning_rate.\n",
    "# It determines how much we learn from each new training data.\n",
    "# 5.6\n",
    "# From the weight table, we can see that features are ranked as follows in terms of weight:\n",
    "# (from most to least important) kids, vryrel, yrsmarr2, unhap, vryunhap, antirel, vryhap,\n",
    "# smerel, yrsmarr1, and the rest has no impact on the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5.7 The two has similar average per-class accuracy, which is just slightly better than 50%\n",
    "# However, Randomized Forest does better in determining affair class while adaboost does better\n",
    "# in determining no-affair class.\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
